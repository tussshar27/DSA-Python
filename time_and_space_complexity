Big O notation: It gives the worst case of the algorithm. Also, it helps in comparing the efficiency of different algorithms and understanding their scalability with large inputs.
NOTE: whenever there are multiple time complexities present in our code then we have to ignore small complexity and always have to choose largest complexity
Example: 
In a single code there are,
print O(1) - assume we have 1 row or  1 billion rows as an input then it will take same constant time to run for both.
loop O(N)  - if the input size is of 1 billion rows then it will also take same time to process it linearly.
Consider:
loop O(N)

To calculate Time complexity:
1. Ignore constant.
2. Take only the highest value as a worst case.

O(1) Constant Time Complexity: whatever the input size, the time taken to process the algorithm will be constant.
Example: print statement inside a function, accessing an array with index arr[2]

O(N) Linear/Linear Search Time Complexity: when input size increases then the time taken to process also increases linearly. 
Mathematically on scale, it is x = y.
Example: using loop to print all the elements of an array or all the chars of a string, when we search all the elements of an array using index then we are performing linear search.

O(Nsquare) Quadratic Time Complexity: It is based on nested loop.
Example: For loop running for N times inside a outer For loop which is aso runnig for N times. So the time complexity becomes O(Nsqaure), traversing through all the cells of matrics (n * n = nsquare).
when the input size increases then the time it takes also increases which is more than O(N).

O(logN): O(LogN) is in between of O(1) and O(N) and it is next to O(1). O(LogN) has better time complexity then O(N) linear time complexity.
Example: Binary Search Algorithm
In Binary search, the input size of an array is split into half each time to search an element.
Suppose array size = 100, so it will split to 50-50, then again it will split to 25-25, till it comes to 1.
Now consider reverse of above operation till our array size i.e. double of input size starting from 1 until 100.
1 -> 1*2 -> 2*2 -> 4*2 -> 25*2 -> 50*2 -> until 100
from above calculation,
1 * 2power(x) = 100
2power(x) = 100
x = logbase2(100)
x = log(N)
x = logN
Here, x = No. of operations
Therefore, No. of operations = logN.

O(NlonN):
Example: All Sorting Algorithms

O(2^N) or O(3powerN) or O(4powerN) Exponential Time Complexity: It has one of the worst and highest time complexity. Due to its highest time complexity so we use Dynamic Programming to make it better.
By using Dynamic Programming, we try to bring time complexity to O(Nsquare) or till O(N).
Example: Recursion, Bruteforce
Suppose N = 4 then 2power4 = 2 * 2 * 2 * 2 = 16

O(N!) N Factorial Time Complexity: It is the worst time complexity. It is worsse than O(2powerN) Exponential Time Complexity.
Example:  Calculation of Permutation.
Suppose N = 4 then N! factorial = 4 * 3 * 2 * 1 = 26

As per the above calculations for O(2powerN) and O(N!), we  can see that the value for O(N!) is bigger than the value for O(2powerN).
So, O(N!) has the worst time complexity than O(2powerN).

O(1) < O(logN) < O(N) < O(Nsquare) < O(N!)



