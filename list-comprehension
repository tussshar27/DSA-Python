#List comprehension offers a shorter syntax when you want to create a new list based on the values of an existing list.
#Basic
l1 = ['tushar','annam']

ls = []
for i in l1:
  ls.append(i.upper())
print(ls)

l2 = [i.upper() for i in l1]
print(l2)

# ----------------------------------
# Boolean - True/False, 0/1, on/off
lb = [True, False, True, True, False, True]
lb2 = []
for i in lb:
  if i == True:
    lb2.append(1)
  else:
    lb2.append(0)
    
print(lb2)

lb3 = [1 if i == True else 0 for i in lb] #compare it with above code
print(lb3)

# ----------------------------------
#String Manipulation
str = "HelloMyNameIsTushar"

#to club each char together, join with empty string
# NOTE: whenever we us list comprehension with string then we have to use .join() with empty string in order to club all chars together.
str3 = "".join([i for i in str])
print(str3)

str5 = ""
for i in str:
  if i.islower():
    str5 += i
  else:
    str5 = str5 + ' ' + i
print(str5.lstrip())

str6 = "".join([i if i.islower() else " " + i for i in str]).lstrip(' ')
print(str6)

#we cannot use elif keyword in between if and else statement inside list comprehension but we can use else keyword:
syntax: if else action condition else

str7 = "".join([i if i.islower() else " " + i.lower() if i in ['N','I'] else " " + i for i in str]).lstrip(' ')
print(str7)




##################################################################################################################

# !pip install pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create a Spark session
spark = SparkSession.builder.appName("CamelToSnakeCase").getOrCreate()

# Sample data (replace with your actual data)
data = [("Alice","Ford", 25), ("Bob","Cruise", 30), ("Carol","Reacher", 28)]
columns = ["FirstName", "LastName","age"]  # Example camel case column names

# Create a DataFrame
df = spark.createDataFrame(data, columns)

# Function to convert camel case to snake case
def camel_to_snake(name):
    return ''.join(['_' + c.upper() if c.isupper() else c for c in name]).lstrip('_')    #NOTE: '_' + c.upper() both together is a one single expression of if condition.

# Identify columns with camel case names
camel_case_columns = [col_name for col_name in df.columns if any(c.isupper() for c in col_name)]  #NOTE: here is we put if condition before for loop then we have to mandatorily give else condition otherwise it will throw syntax error.

# Rename columns
for col_name in camel_case_columns:
    new_col_name = camel_to_snake(col_name)
    df = df.withColumnRenamed(col_name, new_col_name)

# Show the result
df.show()

# Stop the Spark session
spark.stop()
